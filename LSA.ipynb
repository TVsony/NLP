{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b2ee52-6f01-470e-966c-cec0e5ef87a7",
   "metadata": {},
   "source": [
    "# LSA in Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e15d2eff-6d8a-46d2-bc48-c63d7fd193b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\bhimr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "# install 'brown' corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ac2543c-e8e1-4252-b1bd-6d4f676fa2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories = 'news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3137529-13ab-4427-adce-9e7898410969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Seven', 'continent', 'plant'], ['Five', 'ocean', 'planets'], ['Asia', 'largest', 'continent'], ['Pacific', 'Ocean', 'largest'], ['Ocean', 'saline', 'water']]\n"
     ]
    }
   ],
   "source": [
    "TextCorpus = ['Seven continent plant',\n",
    "              'Five ocean planets',\n",
    "              'Asia largest continent',\n",
    "              'Pacific Ocean largest',\n",
    "              'Ocean saline water']\n",
    "\n",
    "text_tokens = [sent.split() for sent in TextCorpus]\n",
    "print(text_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a8431eb-f69d-4707-b120-543c3858def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "transformer = TfidfVectorizer()\n",
    "tfidf = transformer.fit_transform(TextCorpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0736510c-2395-43aa-b537-247ecef11149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.6141889663426562\n",
      "  (0, 1)\t0.49552379079705033\n",
      "  (0, 9)\t0.6141889663426562\n",
      "  (1, 6)\t0.6390704413963749\n",
      "  (1, 4)\t0.42799292268317357\n",
      "  (1, 2)\t0.6390704413963749\n",
      "  (2, 3)\t0.5317722537280788\n",
      "  (2, 0)\t0.6591180018251055\n",
      "  (2, 1)\t0.5317722537280788\n",
      "  (3, 5)\t0.6901592662889633\n",
      "  (3, 3)\t0.5568161504458247\n",
      "  (3, 4)\t0.46220770413113277\n",
      "  (4, 10)\t0.6390704413963749\n",
      "  (4, 8)\t0.6390704413963749\n",
      "  (4, 4)\t0.42799292268317357\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)   # document number , term number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "366b67f1-517e-4a1a-a775-01ccc77b0757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=3)\n",
    "lsa = svd.fit_transform(tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25164126-a937-4dff-b75c-4fda116a59e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.32953882,  0.63854727,  0.63291628],\n",
       "       [ 0.47182536, -0.49558015,  0.32013586],\n",
       "       [ 0.62051217,  0.56429487, -0.25756093],\n",
       "       [ 0.74652819, -0.12447404, -0.46997185],\n",
       "       [ 0.47182536, -0.49558015,  0.32013586]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b5698a-168e-4518-8d15-882a5553b384",
   "metadata": {},
   "source": [
    "# Word2Vec Part-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb50c37b-d9f6-40f7-8b98-2c0285243b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f07a1ab6-f83c-4012-844e-cdc1bbfda3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8d33d3-d023-471d-8461-0c10a6c496b0",
   "metadata": {},
   "source": [
    "# Creating our sentences to train the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5137d2e4-ca8b-44ca-9fce-6f7dc5579d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "TextCorpus = [\"I like Datatrained\",\n",
    "              \"Datatrained has a good ML program\",\n",
    "              \"Datatrained has a good faculty\",\n",
    "              \"Chidri is that good faculty\",\n",
    "              \"I like ML\"\n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eec40e7c-9b5e-4ab4-9f12-f6b3989954ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'like', 'Datatrained']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Assuming TextCorpus is your corpus of text\n",
    "text_corpus = [\"I like Datatrained\"]\n",
    "\n",
    "# Tokenizing the text corpus\n",
    "text_tokens = [sent.split() for sent in text_corpus]\n",
    "\n",
    "# Displaying the first two tokenized sentences\n",
    "print(text_tokens[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2dac76-0dce-4efd-9c6a-c1da0fc03566",
   "metadata": {},
   "source": [
    "# Traininig the word vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b94c67df-93e5-4d1e-841e-2eec80adec37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.2434788e-03  9.3002589e-03 -1.9768013e-04 -1.9674676e-03\n",
      "  4.6040779e-03 -4.0957141e-03  2.7433808e-03  6.9406414e-03\n",
      "  6.0660155e-03 -7.5115245e-03  9.3832621e-03  4.6722624e-03\n",
      "  3.9665061e-03 -6.2441123e-03  8.4608020e-03 -2.1503740e-03\n",
      "  8.8260453e-03 -5.3625237e-03 -8.1302095e-03  6.8252226e-03\n",
      "  1.6713552e-03 -2.1987227e-03  9.5145255e-03  9.4947778e-03\n",
      " -9.7749969e-03  2.5054722e-03  6.1572907e-03  3.8728330e-03\n",
      "  2.0229840e-03  4.3054356e-04  6.7369692e-04 -3.8210077e-03\n",
      " -7.1409447e-03 -2.0890753e-03  3.9242790e-03  8.8195410e-03\n",
      "  9.2600510e-03 -5.9765177e-03 -9.4035845e-03  9.7653260e-03\n",
      "  3.4301181e-03  5.1666196e-03  6.2829559e-03 -2.8045352e-03\n",
      "  7.3234155e-03  2.8305468e-03  2.8712836e-03 -2.3806014e-03\n",
      " -3.1285537e-03 -2.3703722e-03  4.2768526e-03  7.6065306e-05\n",
      " -9.5852101e-03 -9.6664838e-03 -6.1487919e-03 -1.2858211e-04\n",
      "  1.9976101e-03  9.4328849e-03  5.5848937e-03 -4.2911135e-03\n",
      "  2.7834380e-04  4.9648411e-03  7.6990579e-03 -1.1443346e-03\n",
      "  4.3238411e-03 -5.8149449e-03 -8.0426881e-04  8.1008384e-03\n",
      " -2.3602943e-03 -9.6643949e-03  5.7798224e-03 -3.9302041e-03\n",
      " -1.2229916e-03  9.9814879e-03 -2.2565699e-03 -4.7575268e-03\n",
      " -5.3299055e-03  6.9815684e-03 -5.7094269e-03  2.1138685e-03\n",
      " -5.2561709e-03  6.1213090e-03  4.3577305e-03  2.6066082e-03\n",
      " -1.4912279e-03 -2.7463306e-03  8.9938110e-03  5.2162819e-03\n",
      " -2.1627299e-03 -9.4712311e-03 -7.4267737e-03 -1.0638448e-03\n",
      " -7.9502445e-04 -2.5631583e-03  9.6836621e-03 -4.5856525e-04\n",
      "  5.8743320e-03 -7.4483114e-03 -2.5063173e-03 -5.5504031e-03]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the text corpus\n",
    "text_tokens = [sent.split() for sent in TextCorpus]\n",
    "\n",
    "# Training a Word2Vec model\n",
    "model = Word2Vec(text_tokens, min_count=1)\n",
    "\n",
    "# Querying the Word2Vec model for the vector representation of \"ML\"\n",
    "vector_representation = model.wv[\"ML\"]\n",
    "\n",
    "print(vector_representation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9738d28f-19ba-4a66-87a5-a53d52e22969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('that', 0.19912299513816833),\n",
       " ('program', 0.17273388803005219),\n",
       " ('a', 0.17018501460552216),\n",
       " ('like', 0.14595399796962738),\n",
       " ('has', 0.06409836560487747)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"faculty\", topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b0196d-6cc4-48d1-89b5-8144d9aaaffc",
   "metadata": {},
   "source": [
    "## Dataset for the Word2Vec model \n",
    "- Wikipedia Dump: Wikipedia offers dumps of its articles in various languages. You can download and use these dumps to train Word2Vec models on a wide range of topics.\n",
    "- Google News Dataset: Google provides a pre-trained Word2Vec model based on a dataset of Google News articles. While you can use the pre-trained model directly, you can also obtain the dataset and train your own Word2Vec model if needed.\n",
    "- Text8: Text8 is a small subset of the English Wikipedia dump, consisting of the first 100MB of cleaned text from the dump. It's commonly used for testing Word2Vec implementations.\n",
    "- \n",
    "GloVe Datasets: The Global Vectors for Word Representation (GloVe) project offers pre-trained word vectors for different languages and sizes of corpora. You can also obtain the raw data and train your own Word2Vec models.- \n",
    "\n",
    "UMBC WebBase Corpus: The University of Maryland, Baltimore County (UMBC) WebBase Project provides a large text corpus extracted from the web. It contains a wide variety of text genres and can be used to train Word2Vec model- s.\n",
    "\n",
    "Amazon Product Reviews: Amazon offers datasets of product reviews in various categories. These datasets contain text reviews along with ratings and can be used for sentiment analysis and other NLP ta- sks.\n",
    "\n",
    "Twitter Sentiment Analysis Dataset: Several datasets are available for sentiment analysis tasks on Twitter data. These datasets containlabelled labeled with sentiments (positive, negative, neutral) and can be useful for training Word2Vec models for sentiment ana- lysis.\n",
    "\n",
    "MovieLens: The MovieLens dataset contains movie ratings and reviews from users. It can be used to train Word2Vec models for movie recommendation systems or sentiment analysis on movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20406431-ed43-41c6-8607-4d7149e858ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
