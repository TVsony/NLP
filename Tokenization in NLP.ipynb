{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f701f865",
   "metadata": {},
   "source": [
    "# Tokenization \n",
    "\n",
    "Tokenization is the process of breaking down a text into smaller units, such as words or subwords, called tokens. \n",
    "These tokens are the basic building blocks used in natural language processing (NLP) tasks such as text analysis,\n",
    "sentiment analysis, machine translation, and more. Tokenization is a crucial preprocessing step in NLP \n",
    "because it enables computers to understand and process human language by dividing it into meaningful components.\n",
    "\n",
    "There are different approaches to tokenization depending on the specific requirements of the task and \n",
    "the characteristics of the text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b58d743",
   "metadata": {},
   "source": [
    "### Word Tokenization:\n",
    "Word tokenization, also known as word segmentation, divides a text into individual words based on spaces or punctuation\n",
    "marksFor example, the sentence \"Tokenization is important for NLP tasks\" \n",
    "would be tokenized into [\"Tokenization\", \"is\", \"important\", \"for\", \"NLP\", \"tasks\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9573fcb6",
   "metadata": {},
   "source": [
    "### Sentence Tokenization:\n",
    "Sentence tokenization splits a text into individual sentences based on punctuation marks such as periods, exclamation marks,\n",
    "or question marks. For example, the paragraph \"This is sentence one. This is sentence two!\"\n",
    "would be tokenized into [\"This is sentence one.\", \"This is sentence two!\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac398621",
   "metadata": {},
   "source": [
    "### Subword Tokenization:\n",
    "Subword tokenization divides words into smaller meaningful units, such as prefixes, suffixes, or root words.\n",
    "This technique is useful for handling languages with complex morphology or for dealing with out-of-vocabulary words. \n",
    "Examples include Byte-Pair Encoding (BPE) and WordPiece tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca271c8b",
   "metadata": {},
   "source": [
    "### Customized Tokenization:\n",
    "Customized tokenization involves creating tokenization rules specific to the requirements of a particular task or domain. \n",
    "This may include handling special characters, emojis, or domain-specific abbreviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf54ea2",
   "metadata": {},
   "source": [
    "### TweetTokenizer\n",
    "TweetTokenizer is a specific tokenizer provided by NLTK that is designed to handle tokenization of tweets.\n",
    "Tweets often contain non-standard words, hashtags, mentions, URLs, and emojis,\n",
    "which can pose challenges for standard tokenization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5a7718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'the', 'Natural', 'Language', 'Processing']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = 'I am learning the Natural Language Processing'\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3870040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'important', 'for', 'NLP', 'tasks', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Tokenization is important for NLP tasks.\"\n",
    "\n",
    "# Word tokenization\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e755f98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our company annual growth rate is 25.3% .', 'good job Mr.Bhimrao']\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization \n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Our company annual growth rate is 25.3% . good job Mr.Bhimrao\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68321b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'Su', 'Sub', 'Subw', 'Subwo', 'Subwor', 'Subword', 'u', 'ub', 'ubw', 'ubwo', 'ubwor', 'ubword', 'b', 'bw', 'bwo', 'bwor', 'bword', 'w', 'wo', 'wor', 'word', 'o', 'or', 'ord', 'r', 'rd', 'd', 't', 'to', 'tok', 'toke', 'token', 'tokeni', 'tokeniz', 'tokeniza', 'tokenizat', 'tokenizati', 'tokenizatio', 'tokenization', 'o', 'ok', 'oke', 'oken', 'okeni', 'okeniz', 'okeniza', 'okenizat', 'okenizati', 'okenizatio', 'okenization', 'k', 'ke', 'ken', 'keni', 'keniz', 'keniza', 'kenizat', 'kenizati', 'kenizatio', 'kenization', 'e', 'en', 'eni', 'eniz', 'eniza', 'enizat', 'enizati', 'enizatio', 'enization', 'n', 'ni', 'niz', 'niza', 'nizat', 'nizati', 'nizatio', 'nization', 'i', 'iz', 'iza', 'izat', 'izati', 'izatio', 'ization', 'z', 'za', 'zat', 'zati', 'zatio', 'zation', 'a', 'at', 'ati', 'atio', 'ation', 't', 'ti', 'tio', 'tion', 'i', 'io', 'ion', 'o', 'on', 'n', 'i', 'is', 's', 'u', 'us', 'use', 'usef', 'usefu', 'useful', 's', 'se', 'sef', 'sefu', 'seful', 'e', 'ef', 'efu', 'eful', 'f', 'fu', 'ful', 'u', 'ul', 'l', 'f', 'fo', 'for', 'o', 'or', 'r', 'h', 'ha', 'han', 'hand', 'handl', 'handli', 'handlin', 'handling', 'a', 'an', 'and', 'andl', 'andli', 'andlin', 'andling', 'n', 'nd', 'ndl', 'ndli', 'ndlin', 'ndling', 'd', 'dl', 'dli', 'dlin', 'dling', 'l', 'li', 'lin', 'ling', 'i', 'in', 'ing', 'n', 'ng', 'g', 'c', 'co', 'com', 'comp', 'compl', 'comple', 'complex', 'o', 'om', 'omp', 'ompl', 'omple', 'omplex', 'm', 'mp', 'mpl', 'mple', 'mplex', 'p', 'pl', 'ple', 'plex', 'l', 'le', 'lex', 'e', 'ex', 'x', 'w', 'wo', 'wor', 'word', 'words', 'o', 'or', 'ord', 'ords', 'r', 'rd', 'rds', 'd', 'ds', 's', '.']\n"
     ]
    }
   ],
   "source": [
    "# Subword Tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Subword tokenization is useful for handling complex words.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Define a function for subword tokenization\n",
    "def subword_tokenize(word):\n",
    "    # Split the word into subwords (e.g., by adding a space between each character)\n",
    "    subwords = [word[i:j] for i in range(len(word)) for j in range(i + 1, len(word) + 1)]\n",
    "    return subwords\n",
    "\n",
    "# Tokenize each word into subwords\n",
    "subword_tokens = [subword_tokenize(word) for word in words]\n",
    "\n",
    "# Flatten the list of subword tokens\n",
    "flat_subword_tokens = [token for sublist in subword_tokens for token in sublist]\n",
    "\n",
    "# Print the subword tokens\n",
    "print(flat_subword_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df87af24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', 'with', 'some_special_characters', '.', 'It', 'also', 'includes', \"'\", 'quotes', \"'\", 'and', 'emojis', 'ðŸ˜Š', 'ðŸš€', '.']\n"
     ]
    }
   ],
   "source": [
    "# Customized Tokenization\n",
    "import re\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a sample sentence with some_special_characters. It also includes 'quotes' and emojis ðŸ˜ŠðŸš€.\"\n",
    "\n",
    "# Define a custom tokenization function\n",
    "def custom_tokenize(text):\n",
    "    # Define regex patterns for tokenization\n",
    "    patterns = [\n",
    "        r'\\w+',                      # Matches alphanumeric characters\n",
    "        r'[\\u00A1-\\u1FFF\\u2C00-\\uD7FF\\w]+',  # Matches characters from various scripts (e.g., emojis)\n",
    "        r'\\d+',                      # Matches digits\n",
    "        r'[^\\w\\s]'                   # Matches special characters\n",
    "    ]\n",
    "    \n",
    "    # Combine patterns into a single regex pattern\n",
    "    combined_pattern = '|'.join('(?:{})'.format(pattern) for pattern in patterns)\n",
    "    \n",
    "    # Tokenize text using the combined regex pattern\n",
    "    tokens = re.findall(combined_pattern, text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Tokenize the text using the custom tokenization function\n",
    "tokens = custom_tokenize(text)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97cad0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Just', 'bought', 'a', 'new', 'phone', '!', '#excited', 'ðŸ“±', 'ðŸ˜Š']\n"
     ]
    }
   ],
   "source": [
    "# TweetTokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Create a TweetTokenizer instance\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# Sample tweet\n",
    "tweet = \"Just bought a new phone! #excited ðŸ“±ðŸ˜Š\"\n",
    "\n",
    "# Tokenize the tweet\n",
    "tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c60fe9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just', 'bought', 'a', 'new', 'phone', '!', '#excited', 'ðŸ“±', 'ðŸ˜Š']\n"
     ]
    }
   ],
   "source": [
    "# Customize TweetTokenizer\n",
    "tokenizer = TweetTokenizer(reduce_len=True, preserve_case=False)\n",
    "\n",
    "# Tokenize the tweet\n",
    "tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c70faf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'fun', 'and', 'can', 'deal', 'with', 'texts', 'and', 'sound', 'but', 'can', 'not', 'deal', 'with', 'images', 'e', 'have', 'session', 'at', 'e', 'can', 'learn', 'a', 'lot', 'of']\n"
     ]
    }
   ],
   "source": [
    "# RegEx Tokenizations\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "text = \"NLP is fun and can deal with texts and sound, but can not deal with images. We have session at 11 AM!. We can learn a lot of $\"\n",
    "\n",
    "# Print word by all small case and start from small a to z.\n",
    "tokens = regexp_tokenize(text, pattern='[a-z]+')\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db594342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'can',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'texts',\n",
       " 'and',\n",
       " 'sound',\n",
       " 'but',\n",
       " 'can',\n",
       " 'not',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'images',\n",
       " 'e',\n",
       " 'have',\n",
       " 'session',\n",
       " 'at',\n",
       " 'e',\n",
       " 'can',\n",
       " 'learn',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extra quote ' get's you word like can't\n",
    "regexp_tokenize(text,\"[a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbd46e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP', 'W', 'AM', 'W']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print word by word that contains all caps and from caps A to Z \n",
    "regexp_tokenize(text,\"[A-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67875412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP is fun and can deal with texts and sound, but can not deal with images. We have session at 11 AM!. We can learn a lot of $']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every thing in one line \n",
    "regexp_tokenize(text,\"[\\a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1d89566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ', ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '. W',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' 11 AM!. W',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' $']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anything start with caret is not equal \n",
    "regexp_tokenize(text,\"[^a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce3f1fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only numbers\n",
    "regexp_tokenize(text,\"[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71d292a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP is fun and can deal with texts and sound, but can not deal with images. We have session at ',\n",
       " ' AM!. We can learn a lot of $']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without numbers \n",
    "regexp_tokenize(text,\"[^0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e4c250a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# onlt $ symbol \n",
    "regexp_tokenize(text,\"[$]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77fdd56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
